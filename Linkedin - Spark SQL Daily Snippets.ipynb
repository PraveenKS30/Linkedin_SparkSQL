{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-PP3JVFAK:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>readfiles</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1ad8e97b160>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark =  SparkSession\\\n",
    "        .builder\\\n",
    "        .appName('readfiles')\\\n",
    "        .getOrCreate()\n",
    "spark        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----+\n",
      "|train_id|      station| time|\n",
      "+--------+-------------+-----+\n",
      "|     324|San Francisco|7:59a|\n",
      "|     324|  22nd Street|8:03a|\n",
      "|     324|     Millbrae|8:16a|\n",
      "|     324|    Hillsdale|8:24a|\n",
      "|     324| Redwood City|8:31a|\n",
      "|     324|    Palo Alto|8:37a|\n",
      "|     324|     San Jose|9:05a|\n",
      "|     217|       Gilroy|6:06a|\n",
      "|     217|   San Martin|6:15a|\n",
      "|     217|  Morgan Hill|6:21a|\n",
      "|     217| Blossom Hill|6:36a|\n",
      "|     217|      Capitol|6:42a|\n",
      "|     217|       Tamien|6:50a|\n",
      "|     217|     San Jose|6:59a|\n",
      "+--------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## read csv file\n",
    "df = spark.read.csv(\"train_schedule.csv\", header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----+\n",
      "|train_id|      station| time|\n",
      "+--------+-------------+-----+\n",
      "|     324|San Francisco|7:59a|\n",
      "|     324|  22nd Street|8:03a|\n",
      "|     324|     Millbrae|8:16a|\n",
      "|     324|    Hillsdale|8:24a|\n",
      "|     324| Redwood City|8:31a|\n",
      "|     324|    Palo Alto|8:37a|\n",
      "|     324|     San Jose|9:05a|\n",
      "|     217|       Gilroy|6:06a|\n",
      "|     217|   San Martin|6:15a|\n",
      "|     217|  Morgan Hill|6:21a|\n",
      "|     217| Blossom Hill|6:36a|\n",
      "|     217|      Capitol|6:42a|\n",
      "|     217|       Tamien|6:50a|\n",
      "|     217|     San Jose|6:59a|\n",
      "+--------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Another way of reading same file\n",
    "df = spark.read.option(\"header\",\"True\").csv(\"train_schedule.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark provides this functionality of not to define schema and tries to infer it automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- train_id: integer (nullable = true)\n",
      " |-- station: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"train_schedule.csv\", header = True, inferSchema = True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StructType and StructField combination help you to define your own schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import IntegerType, StringType, TimestampType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your own schema by referring columns and casting it to required type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = StructType([\n",
    "                            StructField('train_id', IntegerType(), True),\n",
    "                            StructField('station', StringType(), True),\n",
    "                            StructField('time', TimestampType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read file by referring your custom schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"train_schedule.csv\", schema = data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- train_id: integer (nullable = true)\n",
      " |-- station: string (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Spark dataframe to Pandas dataframe using toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.csv(\"train_schedule.csv\", header = True)\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(train_id='324', station='San Francisco', time='7:59a'),\n",
       " Row(train_id='324', station='22nd Street', time='8:03a')]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## read spark dataframe\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdDF = df.toPandas()\n",
    "type(pdDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_id</th>\n",
       "      <th>station</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>324</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>7:59a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324</td>\n",
       "      <td>22nd Street</td>\n",
       "      <td>8:03a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  train_id        station   time\n",
       "0      324  San Francisco  7:59a\n",
       "1      324    22nd Street  8:03a"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read pandas dataframe\n",
    "pdDF.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Pandas dataframe to Spark dataframe using spark.createDataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkDF = spark.createDataFrame(pdDF)\n",
    "type(sparkDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert RDD to Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[312] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddSc = sc.parallelize([1,2,3])\n",
    "rddSc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing SQL statment on File by registering it as tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv('train_schedule.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.createOrReplaceTempView('data_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----+\n",
      "|train_id|      station| time|\n",
      "+--------+-------------+-----+\n",
      "|     324|San Francisco|7:59a|\n",
      "|     324|  22nd Street|8:03a|\n",
      "|     324|     Millbrae|8:16a|\n",
      "|     324|    Hillsdale|8:24a|\n",
      "|     324| Redwood City|8:31a|\n",
      "+--------+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'SELECT * FROM data_table'\n",
    "spark.sql(query).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL 6 : Chek tables presence using catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary table is created using createOrReplaceTempView\n",
    "data_table = data.createOrReplaceTempView('data_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='data_table', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop your table\n",
    "spark.catalog.dropTempView('data_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check again\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSQL 7\n",
    "\n",
    "### Caching and UnCaching tables\n",
    "\n",
    "#### To cache Table/View : spark.catalog.cacheTable()\n",
    "#### To uncache Table/view : spark.catalog.uncacheTable()\n",
    "#### To check wheter cached or not : spark.catalog.isCached()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables:\n",
      " [Table(name='data_table', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n",
      "table1 is cached:  True\n",
      "table1 is cached:  False\n"
     ]
    }
   ],
   "source": [
    "# List the tables\n",
    "print(\"Tables:\\n\", spark.catalog.listTables())\n",
    "\n",
    "# Cache table1 and Confirm that it is cached\n",
    "spark.catalog.cacheTable('data_table')\n",
    "print(\"table1 is cached: \", spark.catalog.isCached('data_table'))\n",
    "\n",
    "# Uncache table1 and confirm that it is uncached\n",
    "spark.catalog.uncacheTable('data_table')\n",
    "print(\"table1 is cached: \", spark.catalog.isCached('data_table'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #SparkSQL 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching Spark DataFrames\n",
    "#### To cache : cache() or persist()\n",
    "#### To uncache : unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv('train_schedule.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.cache()\n",
    "data.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.unpersist()\n",
    "data.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.persist(storageLevel= pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "data.is_cached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSQL 9\n",
    "\n",
    "#### Query data using DataFrame API and Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----+\n",
      "|train_id|      station| time|\n",
      "+--------+-------------+-----+\n",
      "|     324|San Francisco|7:59a|\n",
      "|     324|  22nd Street|8:03a|\n",
      "|     324|     Millbrae|8:16a|\n",
      "|     324|    Hillsdale|8:24a|\n",
      "|     324| Redwood City|8:31a|\n",
      "+--------+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+\n",
      "|train_id|count(station)|\n",
      "+--------+--------------+\n",
      "|     217|             7|\n",
      "|     324|             7|\n",
      "+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupBy('train_id').agg({'station':'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+\n",
      "|train_id|count(station)|\n",
      "+--------+--------------+\n",
      "|     217|             7|\n",
      "|     324|             7|\n",
      "+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.createOrReplaceTempView('data_table')\n",
    "query ='SELECT train_id, count(station) FROM data_table GROUP BY train_id'\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GlobalTempView"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporary views in Spark SQL are session-scoped and will disappear<br> \n",
    "if the session that creates it terminates. If you want to have a temporary view<br>\n",
    "that is shared among all sessions and keep alive until the Spark application terminates,<br>\n",
    "you can create a global temporary view.\n",
    "\n",
    "* Don't forget to include **global_temp** while referring global table/view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.createGlobalTempView('dataglb_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----+\n",
      "|train_id|      station| time|\n",
      "+--------+-------------+-----+\n",
      "|     324|San Francisco|7:59a|\n",
      "|     324|  22nd Street|8:03a|\n",
      "|     324|     Millbrae|8:16a|\n",
      "|     324|    Hillsdale|8:24a|\n",
      "|     324| Redwood City|8:31a|\n",
      "+--------+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'SELECT * FROM global_temp.dataglb_tbl'\n",
    "spark.sql(query).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common confusion happens while creating dataframe in Pandas and Spark.\n",
    "* To create dataframe in pandas use: DataFrame()\n",
    "* To create data dataframe in spark : createDataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_data = pd.DataFrame( \n",
    "                        {'Name' : 'Sumo', 'City':'UK'},\\\n",
    "                        {'Name' : 'Mark', 'City':'US'}\n",
    "                    )\n",
    "print('******* Pandas DatFrame*******')\n",
    "print(pd_data.head())\n",
    "\n",
    "print('******* Spark DatFrame*******')\n",
    "spark_data= spark.createDataFrame(pd_data, schema=['Name', 'City'])\n",
    "print(spark_data.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSQL 12\n",
    "\n",
    "Format : <b> createDataFrame(data, schema=None, verifySchema=True)</b>\n",
    "\n",
    "Spark DataFrame can only be created using <br>\n",
    "<b>RDD, a list or a pandas.DataFrame.</b><br>\n",
    "\n",
    "When schema is None, it will try to infer the schema<br>\n",
    "(column names and types) from data, which should be \n",
    "<b>an RDD of Row, or namedtuple, or dict</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "\n",
    "#define schema, if you don't pass it then spark will try\n",
    "#to infer it automatically\n",
    "schema = StructType([\n",
    "                    StructField('Name' , StringType(), True),\n",
    "                    StructField('City', StringType(), True)\n",
    "                ])\n",
    "\n",
    "spark_data = spark.createDataFrame([Row('Sumo','UK'),\n",
    "                                    Row('Mark','US')], schema=schema)\n",
    "#print(spark_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing partitions of Spark DataFrame\n",
    "* To check the existing partition : getNumPartitions()\n",
    "* To change partition : repartition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+--------+\n",
      "|train_id|\n",
      "+--------+\n",
      "|     217|\n",
      "|     324|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"train_schedule.csv\", header = True)\n",
    "print(type(df))\n",
    "df.select('train_id').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataframe has partition: 1\n",
      "The dataframe has new partition: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"The dataframe has partition:\", df.rdd.getNumPartitions())\n",
    "# Chagnging partition to 2\n",
    "df = df.repartition(2, 'train_id')\n",
    "print(\"The dataframe has new partition:\", df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain plan dataframes and tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
